{
  "id": "w2v_job",
  "type": "script",
  "maxRows": 1,
  "script": "import sys.process._ \n import com.lucidworks.searchhub.analytics.AnalyzerUtils._ \n import com.lucidworks.searchhub.analytics._ \n import org.apache.spark.sql.SQLContext \n import java.io._ \n import com.lucidworks.apollo.pipeline.index.stages.searchhub.w2v.PrepareFile \n val opts = Map(\"zkhost\" -> \"localhost:9983\", \"collection\" -> \"lucidfind\", \"query\" -> \"*:*\",\"fields\" -> \"id,body,title,subject,publishedOnDate,project,content\") \n val mailDF = sqlContext.read.format(\"solr\").options(opts).load \n mailDF.cache() \n mailDF.count() \n val textColumnName = \"body\" \n val tokenizer = analyzerFn(noHTMLstdAnalyzerSchema) \n val vectorizer = TfIdfVectorizer.build(mailDF, tokenizer, textColumnName) \n val vectorizedMail = TfIdfVectorizer.vectorize(mailDF, vectorizer, textColumnName) \n vectorizedMail.cache() \n val filedir=new File(\"modelId\") \n filedir.mkdir() \n val idfMapData=new File(filedir,\"idfMapData\") \n if(idfMapData.exists)\"rm -rf modelId/idfMapData\"! \n sc.getConf.set(\"spark.kryoserializer.buffer.max\",\"512m\") \n sc.parallelize(vectorizer.idfs.toSeq).saveAsTextFile(\"modelId/idfMapData\") \n val w2vModel = ManyNewsgroups.buildWord2VecModel(vectorizedMail, tokenizer, textColumnName) \n val w2vModelFile=new File(\"modelId/w2vModelData\") \n if(w2vModelFile.exists)\"rm -rf modelId/w2vModelData\"! \n w2vModel.save(sc, \"modelId/w2vModelData\") \n PrepareFile.createZipFile \n \"curl -u admin:password123 -X DELETE http://localhost:8764/api/apollo/blobs/modelId666\" ! \n \n \"curl -u admin:password123 -X PUT --data-binary @modelId.zip -H Content-type:application/zip http://localhost:8764/api/apollo/blobs/modelId666?modelType=com.lucidworks.apollo.pipeline.index.stages.searchhub.w2v.W2VRelatedTerms\" !"
}