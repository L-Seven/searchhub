{
  "type":"script",
  "id":"w2v_job",
  "maxRows":1,
  "script":"import sys.process._ \n import com.lucidworks.searchhub.analytics.AnalyzerUtils._ \n  import com.lucidworks.searchhub.analytics._ \n import org.apache.spark.sql.SQLContext \n  import java.io._ \n import com.lucidworks.apollo.pipeline.index.stages.searchhub.w2v.PrepareFileModified \n import com.lucidworks.spark.analysis.LuceneTextAnalyzer \n import org.apache.spark.storage.StorageLevel._ \n val opts = Map(\"zkhost\" -> \"localhost:9983\", \"collection\" -> \"lucidfind\", \"query\" -> \"*:*\",\"fields\" -> \"id,body,title,subject,publishedOnDate,project,content\") \n val mailDF = sqlContext.read.format(\"solr\").options(opts).load \n mailDF.cache() \n mailDF.count() \n   val finalStopwordsRemovalSchema =\"\"\"{ \"analyzers\": [ { \"name\": \"NoStdTokLowerStop\", \"charFilters\": [ { \"type\": \"htmlstrip\" } ], \"tokenizer\": { \"type\": \"pattern\", \"pattern\":\"\\W|\\d\", \"group\":\"-1\" }, \"filters\": [ { \"type\": \"lowercase\" }, { \"type\": \"stop\", \"ignoreCase\":\"true\", \"words\":\"stopwords.txt\" }] }], \"fields\": [{ \"regex\": \".+\", \"analyzer\": \"NoStdTokLowerStop\" } ]} \"\"\".stripMargin \n val textColumnName = \"body\" \n val tokenizer = analyzerFn(finalStopwordsRemovalSchema) \n val vectorizer = TfIdfVectorizer.build(mailDF, tokenizer, textColumnName) \n val vectorizedMail = TfIdfVectorizer.vectorize(mailDF, vectorizer, textColumnName) \n vectorizedMail.cache() \n val filedir=new File(\"modelId\") \n if(filedir.exists)\"rm -rf modelId\"! \n filedir.mkdir() \n val idfMapData=new File(filedir,\"idfMapData\") \n sc.parallelize(vectorizer.idfs.toSeq).saveAsTextFile(\"modelId/idfMapData\") \n val w2vModel = ManyNewsgroups.buildWord2VecModel(vectorizedMail, tokenizer, textColumnName) \n val w2vModelFile=new File(\"modelId/w2vModelData\") \n w2vModel.save(sc, \"modelId/w2vModelData\") \n PrepareFileModified.createZipAndSendFile"
}