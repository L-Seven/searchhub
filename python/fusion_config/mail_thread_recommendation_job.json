{
  "id": "mail_thread_recommendation_job",
  "type": "script",
  "maxRows": 1,
  "script": "import org.apache.spark.sql.{Row, DataFrame}\n case class UserId(id: String)\n case class ItemId(id: String)\n case class Pref(userId: UserId, itemId: ItemId, weight: Double)\n \n case class ItemSim(itemId1: ItemId, itemId2: ItemId, weight: Double)\n case class UnStructSim(itemId1: String, itemId2: String, weight_d: Double)\n object SimpleTwoHopRecommender extends Serializable {\n \n def itemRecs(userItemMatrix: DataFrame, userIdCol: String, itemIdCol: String, weightCol: String,\n recsPerItem: Int = 10, outerProductLimit: Int = 100): DataFrame = {\n val toPref = (row: Row) =>\n Pref(UserId(row.getAs[String](userIdCol)), ItemId(row.getAs[String](itemIdCol)), row.getAs[Double](weightCol))\n val prefMatrix = userItemMatrix.rdd.map(toPref)\n val matrixProduct = prefMatrix.groupBy(_.userId).flatMap { case (userId, prefs) =>\n val topPrefs = prefs.toList.sortBy(-_.weight).take(outerProductLimit)\n for {\n pref1 <- prefs\n pref2 <- prefs\n if pref1.itemId != pref2.itemId\n } yield {\n ItemSim(pref1.itemId, pref2.itemId, pref1.weight * pref2.weight)\n }\n }\n val matrixSumReduced = matrixProduct.groupBy(sim => (sim.itemId1, sim.itemId2)).map { case (_, sims: Iterable[ItemSim]) =>\n sims.reduce { (s1: ItemSim, s2: ItemSim) => s1.copy(weight = s1.weight + s2.weight) }\n }\n val recs = matrixSumReduced.groupBy(_.itemId1).mapValues(_.toList.sortBy(-_.weight).take(recsPerItem)).flatMap(_._2)\n val unStrucRecs = recs.map(s => UnStructSim(s.itemId1.id, s.itemId2.id, s.weight))\n import userItemMatrix.sqlContext.implicits._\n unStrucRecs.toDF.withColumnRenamed(\"itemId1\", \"rec_for_\" + itemIdCol).withColumnRenamed(\"itemId2\", itemIdCol)\n }\n }\n val opts = Map(\"zkhost\" -> \"localhost:9983\", \"collection\" -> \"lucidfind_signals_aggr\", \"query\" -> \"*:*\")\n val tmpDF = sqlContext.read.format(\"solr\").options(opts).load\n val recs = SimpleTwoHopRecommender.itemRecs(tmpDF, \"from_email_s\", \"subject_simple_s\", \"weight_d\", 10, 100)\n import sqlContext.implicits._\n import org.apache.spark.sql.functions._\n val finalRecs = recs\n finalRecs.write.format(\"solr\").options(Map(\"zkhost\" -> \"localhost:9983\", \"collection\" -> \"lucidfind_thread_recs\")).mode(org.apache.spark.sql.SaveMode.Overwrite).save\n com.lucidworks.spark.util.SolrSupport.getCachedCloudClient(\"localhost:9983\").commit(\"lucidfind_thread_recs\")\n"
}
