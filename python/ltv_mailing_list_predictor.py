# -*- coding: utf-8 -*-
from __future__ import division
import pysolr
import requests
import itertools
from pandas import Series, DataFrame
import pandas as pd
import numpy as np
import scipy
import sys
import matplotlib.pyplot as plt
reload(sys)
sys.setdefaultencoding('utf8')

# Specify Solr shard - We want to take our data from the lucidfind collection - lucidfind_shard1_replica1
solr = pysolr.Solr('http://localhost:8983/solr/lucidfind_shard1_replica1/',timeout=10)

#########################################################################################################################################################

# INPUTS TO SPECIFY PRIOR TO RUNNING

# Specify Mailing List of interest
# e.g let's look at the Hadoop common dev mailing list            
mailing_list = "mailing-list-hadoop-hadoop-common-dev"

# We want to see how well the model performs for predicting the number of mails for a specific contributor.
# For example, let's look at 'Doug Cutting (JIRA)'
contributor = 'Doug Cutting (JIRA)' 

#########################################################################################################################################################

# Need to find those docs without body field and delete them
missing_body_docs = solr.search('_lw_data_source_s:{ml} AND -body:[* TO *]'.format(ml=mailing_list),rows=100000)

for i in range(len(missing_body_docs.docs)):
    
    hash_id = str(missing_body_docs.docs[i]['hash_id'])
    hash_del = hash_id[3:][:-2] #removing square brackets
    requests.get('http://localhost:8983/solr/lucidfind_shard1_replica1/update?stream.body=%3Cdelete%3E%3Cquery%3Ehash_id%3A{HASH}%3C/query%3E%3C/delete%3E&commit=true'.format(HASH = hash_del))

# Need to find those docs without subject field and delete them
missing_subject_docs = solr.search('_lw_data_source_s:{ml} AND -subject:[* TO *]'.format(ml=mailing_list),rows=100000)

for i in range(len(missing_subject_docs.docs)):
    
    hash_id = str(missing_subject_docs.docs[i]['hash_id'])
    hash_del = hash_id[3:][:-2] #removing square brackets
    requests.get('http://localhost:8983/solr/lucidfind_shard1_replica1/update?stream.body=%3Cdelete%3E%3Cquery%3Ehash_id%3A{HASH}%3C/query%3E%3C/delete%3E&commit=true'.format(HASH = hash_del))

params = {
  'facet': 'on',
  'facet.field': 'author_facet',
  'facet.limit':  -1
}

results = solr.search('_lw_data_source_s:{ml}'.format(ml=mailing_list),**params)

D = results.facets['facet_fields']['author_facet']

# Turning list D into a dict 
D = dict(itertools.izip_longest(*[iter(D)] * 2, fillvalue=""))

# Creating a dict of all those contributors with 1 or greater mails
active_contribs = {key:value for key, value in D.items() if value > 0}

#Create dependent variable (number of mails sent)
y = active_contribs.values()

#########################################################################################################################################################

# VARIABLE GENERATION

#########################################################################################################################################################

#############################
# Average Length of Mail (x1)
#############################

# Want to examine if there is a relationship between average length of email and number of emails sent

# to extract average length of email sent by each committer
# we must loop through every author, get the length_l of every email sent, and divide by the total number of emails sent by that person

average_length_l = [] 

for i in range(len(active_contribs.keys())): #looping through the list of (7618) authors 
    
    """as there are some non-english characters in 
    certain contributors names we must change the 
    input string type to utf-8 in such instances"""
    if isinstance(active_contribs.keys()[i], str):
        active_contribs.keys()[i] = unicode(active_contribs.keys()[i], "utf-8")
    s = '_lw_data_source_s:{ml} AND author:"{author}"'
    solr_results = solr.search(s.format(ml=mailing_list,author=active_contribs.keys()[i]),rows=active_contribs.values()[i])
    author_docs = solr_results.docs
    
    a = []
    
    for j in range(len(author_docs)): #looping through the docs of each author
         
        length = author_docs[j]['length_l']
        a.append(length)
        
    av_length = sum(a) / len(solr_results.docs)
    
    average_length_l.append(av_length)

x1 = average_length_l


#########################################################################################################################################################

#################################
# IsBot (x2) - indicator variable
#################################

# Are the mails of that author generated by a bot or not
isbot = [] 

for i in range(len(active_contribs.keys())):
    
    if isinstance(active_contribs.keys()[i], str):
        active_contribs.keys()[i] = unicode(active_contribs.keys()[i], "utf-8")
    s = '_lw_data_source_s:{ml} AND author:"{author}"'
    solr_results = solr.search(s.format(ml=mailing_list,author=active_contribs.keys()[i]),rows=active_contribs.values()[i])
    author_docs = solr_results.docs
    if author_docs[0]['isBot'] == True:
        isbot.append(1)
    elif author_docs[0]['isBot'] == False:
        isbot.append(0)
    
x2 = isbot  


#########################################################################################################################################################

#######################################################
# Number of mails which contain a question in body (x3)
#######################################################

# To get a basic sentiment of whther a contributor is generally asking or answering questions
# We look at the total number of mails containing at least one question for each author

num_questions = [] 

for i in range(len(active_contribs.keys())):
    
    if isinstance(active_contribs.keys()[i], str):
        active_contribs.keys()[i] = unicode(active_contribs.keys()[i], "utf-8")
    s = '_lw_data_source_s:{ml} AND author:"{author}"'
    solr_results = solr.search(s.format(ml=mailing_list,author=active_contribs.keys()[i]),rows=active_contribs.values()[i])
    author_docs = solr_results.docs
    
    questions = []
    
    for j in range(len(solr_results.docs)):
         
        body = author_docs[j]['body']
        torf = '?' in body
        if torf == True: 
            questions.append(1)  
              
    num_questions.append(sum(questions))        

x3 = num_questions

#########################################################################################################################################################
        
##########################################################
# Number of mails which contain a question in subject (x4)
##########################################################            

# A further gauge of whether an author is asking or answering questions
# Number of mails which contain a subject which is a question

num_subject_qs = [] 

for i in range(len(active_contribs.keys())):
    
    if isinstance(active_contribs.keys()[i], str):
        active_contribs.keys()[i] = unicode(active_contribs.keys()[i], "utf-8")
    s = '_lw_data_source_s:{ml} AND author:"{author}"'
    solr_results = solr.search(s.format(ml=mailing_list,author=active_contribs.keys()[i]),rows=active_contribs.values()[i])
    author_docs = solr_results.docs
    
    subject_questions = []
    
    for j in range(len(solr_results.docs)):
         
        subject = author_docs[j]['subject']
        torf = '?' in subject
        if torf == True: 
            subject_questions.append(1)  
              
    num_subject_qs.append(sum(subject_questions))        

x4 = num_subject_qs

#########################################################################################################################################################

# Generally, a contributor who asks questions will contain some kind of variation of thank you in their mail
# Count how many emails from an author contains some kind of variation of thank you

###########################################################################
# Number of mails which contain some form of "thanks" in body field (x5)
###########################################################################  

num_thanks = [] 

for i in range(len(active_contribs.keys())):
    
    if isinstance(active_contribs.keys()[i], str):
        active_contribs.keys()[i] = unicode(active_contribs.keys()[i], "utf-8")
    s = '_lw_data_source_s:{ml} AND author:"{author}"'
    solr_results = solr.search(s.format(ml=mailing_list,author=active_contribs.keys()[i]),rows=active_contribs.values()[i])
    author_docs = solr_results.docs
    
    var_1 = []
    var_2 = []
    var_3 = []
  
    for j in range(len(solr_results.docs)):
         
        body = author_docs[j]['body']
        torf = 'thanks' in body.lower()
        if torf == True: 
            var_1.append(1)
            
        torf2 = 'thank' in body.lower()
        if torf2 == True: 
            var_2.append(1)

        torf3 = 'thx' in body.lower()
        if torf3 == True: 
            var_3.append(1)
            
        tot = var_1 + var_2 + var_3            
              
    num_thanks.append(sum(tot))        

x5 = num_thanks

#########################################################################################################################################################

#FIXED

# the mention of "fixed" in either the subject or body fields

##################################################################
# Number of mails which contain "fixed" in body/subject field (x6)
##################################################################

num_fixed = [] 

for i in range(len(active_contribs.keys())):
    
    if isinstance(active_contribs.keys()[i], str):
        active_contribs.keys()[i] = unicode(active_contribs.keys()[i], "utf-8")
    s = '_lw_data_source_s:{ml} AND author:"{author}"'
    solr_results = solr.search(s.format(ml=mailing_list,author=active_contribs.keys()[i]),rows=active_contribs.values()[i])
    author_docs = solr_results.docs
    
    body_fixed = []
    subject_fixed = []
  
    for j in range(len(solr_results.docs)):
         
        body = author_docs[j]['body']
        torf = 'fixed' in body.lower()
        if torf == True: 
            body_fixed.append(1)
        
        subject = author_docs[j]['subject']
        torf2 = 'fixed' in subject.lower()
        if torf2 == True: 
            subject_fixed.append(1)

        tot = subject_fixed + body_fixed           
              
    num_fixed.append(sum(tot))        

x6 = num_fixed

#########################################################################################################################################################

#COMMIT

# the mention of commit in either the subject or body fields

##########################################################################################################
# Number of mails which contain variation of 'commit'/'committer'/'comitted' in body or subject field (x7)
##########################################################################################################

num_commit= [] 

for i in range(len(active_contribs.keys())):
    
    if isinstance(active_contribs.keys()[i], str):
        active_contribs.keys()[i] = unicode(active_contribs.keys()[i], "utf-8")
    s = '_lw_data_source_s:{ml} AND author:"{author}"'
    solr_results = solr.search(s.format(ml=mailing_list,author=active_contribs.keys()[i]),rows=active_contribs.values()[i])
    author_docs = solr_results.docs
    
    body_commit = []
    body_committer = []
    body_committed = []
    subject_commit = []
    subject_committed = []
    subject_committer = []
  
    for j in range(len(solr_results.docs)):
         
        body = author_docs[j]['body']
        torf = 'commit' in body.lower()
        if torf == True: 
            body_commit.append(1)
            
        torf2 = 'committer' in body.lower()
        if torf2 == True: 
            body_committer.append(1)
            
        torf3 = 'committed' in body.lower()
        if torf3 == True: 
            body_committed.append(1)
        
        subject = author_docs[j]['subject']
        torf4 = 'commit' in subject.lower()
        if torf4 == True: 
            subject_commit.append(1)
            
        torf5 = 'committer' in subject.lower()
        if torf5 == True: 
            subject_committer.append(1)
            
        torf6 = 'committed' in subject.lower()
        if torf6 == True: 
            subject_committed.append(1)

        tot =  body_commit+body_committer+body_committed+subject_commit+subject_committed+subject_committer         
              
    num_commit.append(sum(tot))        

x7 = num_commit

#########################################################################################################################################################                                                     

# Dataframe of dependent and independent variables 

# Dataframe containing all variables
d = {'y' : pd.Series(y,index = active_contribs.keys()),
'x1':pd.Series(x1,index = active_contribs.keys()),
'x2':pd.Series(x2,index = active_contribs.keys()),
'x3':pd.Series(x3,index = active_contribs.keys()),
'x4':pd.Series(x4,index = active_contribs.keys()),
'x5':pd.Series(x5,index = active_contribs.keys()),
'x6':pd.Series(x6,index = active_contribs.keys()),
'x7':pd.Series(x7,index = active_contribs.keys())}
df = pd.DataFrame(d)

df.corr()
# Variables x1 and x2 displaying v little correlation with y

# Dataframe containing x3,x4,x5,x6,x7
d2 = {'y' : pd.Series(y,index = active_contribs.keys()),
'x3':pd.Series(x3,index = active_contribs.keys()),
'x4':pd.Series(x4,index = active_contribs.keys()),
'x5':pd.Series(x5,index = active_contribs.keys()),
'x6':pd.Series(x6,index = active_contribs.keys()),
'x7':pd.Series(x7,index = active_contribs.keys())}
df2 = pd.DataFrame(d2)

df2.corr()
# High degree of multicollinearity present - becomes particularly problematic when we come to look at regression methods
# Unable to satisfactorily separate out the individual effects that highly correlated variables have on the dependent variable. 
# Two highly correlated variables will generally increase or decrease together 
# resultantly difficult to correctly deduce how each of these variables individually affect the response variable.

#########################################################################################################################################################                                                     

# Training and test set - variables x3,x4,x5,x6,x7

X = {'x3':pd.Series(x3,index = active_contribs.keys()),
'x4':pd.Series(x4,index = active_contribs.keys()),
'x5':pd.Series(x5,index = active_contribs.keys()),
'x6':pd.Series(x6,index = active_contribs.keys()),
'x7':pd.Series(x7,index = active_contribs.keys())}
X = pd.DataFrame(X)

y = {'y':pd.Series(y,index = active_contribs.keys())}
y = pd.DataFrame(y)

# divide the data set up into a training and test set 
# 75% Training / 25% Test
# generate a random permuation of the range 0 to 5281 and then pick off the first 3961 for the training set

import numpy.random as npr
npr.seed(123)
train_select = npr.permutation(range(len(active_contribs.keys())))
X_train = X.ix[train_select[:int(round(len(active_contribs.keys())*0.75))],:].reset_index(drop=True)
X_test = X.ix[train_select[int(round(len(active_contribs.keys())*0.75)):],:].reset_index(drop=True)
y_train = y.ix[train_select[:int(round(len(active_contribs.keys())*0.75))]].reset_index().y
y_test = y.ix[train_select[int(round(len(active_contribs.keys())*0.75)):]].reset_index().y      

#########################################################################################################################################################

X_full = {'x1':pd.Series(x3,index = active_contribs.keys()),
'x2':pd.Series(x4,index = active_contribs.keys()),
'x3':pd.Series(x3,index = active_contribs.keys()),
'x4':pd.Series(x4,index = active_contribs.keys()),
'x5':pd.Series(x5,index = active_contribs.keys()),
'x6':pd.Series(x6,index = active_contribs.keys()),
'x7':pd.Series(x7,index = active_contribs.keys())}
X_full = pd.DataFrame(X_full)

X_full_train = X_full.ix[train_select[:int(round(len(active_contribs.keys())*0.75))],:].reset_index(drop=True)
X_full_test = X_full.ix[train_select[int(round(len(active_contribs.keys())*0.75)):],:].reset_index(drop=True)

#########################################################################################################################################################




#########################################################################################################################################################

# MODEL CONSTRUCTION 

#########################################################################################################################################################

# Building the most accurate statistical model
# We shall use Mean Squared Error to make a judgement of the most accurate model - looking for model with lowest MSE

#########################################################################################################################################################

# Ridge Regression

# tackles the problem of unstable regression coefficients when predictors are highly correlated

# Ridge regression performs particularly well when there is a subset of true coefficients that are small or even zero. It doesn’t do as
# well when all of the true coefficients are moderately large; however, in this case it can still outperform linear regression over a pretty
# narrow range of (small) λ values

from sklearn.linear_model import Ridge

# Ridge Regression with intercept
ridge_int = Ridge(fit_intercept=True, normalize=True)
ridge_int.fit(X_train, y_train) 
ridge_int.score(X_train, y_train, sample_weight=None)
ridge_int_test_pred = ridge_int.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,ridge_int_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Compute mean squared error
from sklearn.metrics import mean_squared_error
MSE_ridge_int = mean_squared_error(y_test,ridge_int_test_pred)

# Ridge Regression with no intercept
ridge_noint = Ridge(fit_intercept=False, normalize=True)
ridge_noint.fit(X_train, y_train) 
ridge_noint.score(X_train, y_train, sample_weight=None)
ridge_noint_test_pred = ridge_noint.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,ridge_noint_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Compute mean squared error
MSE_ridge_noint = mean_squared_error(y_test,ridge_noint_test_pred)

# RidgeCV implements ridge regression with built-in cross-validation of the alpha parameter. 
# The object works in the same way as GridSearchCV except that it defaults to Generalized Cross-Validation (GCV), an efficient form of leave-one-out cross-validation.
# We utilise 10-fold Cross-validation

from sklearn.linear_model import RidgeCV

ridgecv_int = linear_model.RidgeCV(cv=10,normalize=True,fit_intercept=True)
ridgecv_int.fit(X_train, y_train)
# Predict
ridgecv_int_test_pred = ridgecv_int.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,ridgecv_int_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Compute mean squared error
MSE_ridgecv_int = mean_squared_error(y_test,ridgecv_int_test_pred)

ridgecv_noint = linear_model.RidgeCV(cv=10,normalize=True,fit_intercept=False)
ridgecv_noint.fit(X_train, y_train)
# Predict
ridgecv_noint_test_pred = ridgecv_noint.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,ridgecv_noint_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Compute mean squared error
MSE_ridgecv_noint = mean_squared_error(y_test,ridgecv_noint_test_pred)

#n_alphas = 200
#alphas = np.logspace(2, 10, n_alphas)
#ridge = linear_model.Ridge(fit_intercept=True)

#coefs = []
#for a in alphas:
#    ridge.set_params(alpha=a)
#    ridge.fit(X_full_train, y_train)
#    coefs.append(ridge.coef_)

# Display results

#ax = plt.gca()
#ax.set_color_cycle(['b', 'r', 'g', 'c', 'k', 'y', 'm'])

#ax.plot(alphas, coefs)
#ax.set_xscale('log')
#ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis
#plt.xlabel('alpha')
#plt.ylabel('weights')
#plt.title('Ridge coefficients as a function of the regularization')
#plt.axis('tight')
#plt.show()

######################################################################################################################

# Lasso

#from sklearn import cross_validation, linear_model
#lasso = linear_model.Lasso()
#alphas = np.logspace(-10, 2, n_alphas)

#scores = list()
#scores_std = list()

#for alpha in alphas:
#    lasso.alpha = alpha
#    this_scores = cross_validation.cross_val_score(lasso, X_train, y_train, n_jobs=1)
#    scores.append(np.mean(this_scores))
#    scores_std.append(np.std(this_scores))

#plt.semilogx(alphas, scores)
# Error lines showing +/- std. errors of the scores
#plt.semilogx(alphas, np.array(scores) + np.array(scores_std) / np.sqrt(len(X)),'b--')
#plt.semilogx(alphas, np.array(scores) - np.array(scores_std) / np.sqrt(len(X)),'b--')
#plt.ylabel('CV score')
#plt.xlabel('alpha')
#plt.axhline(np.max(scores), linestyle='--', color='.5')
#optimal_alpha_lasso = alphas[scores.index(np.max(scores))]
#plt.axvline(optimal_alpha_lasso, linestyle='--', color='.5')

from sklearn.linear_model import Lasso
lasso = linear_model.Lasso(fit_intercept=True)
lasso.fit(X_train, y_train)
# Predict
lasso_test_pred = lasso.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,lasso_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Compute mean squared error
MSE_lasso = mean_squared_error(y_test,lasso.predict(X_test))

lasso_noint = linear_model.Lasso(fit_intercept=False)
lasso_noint.fit(X_train, y_train)
# Predict
lasso_noint_test_pred = lasso_noint.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,lasso_noint_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Compute mean squared error
MSE_lasso_noint = mean_squared_error(y_test,lasso_noint.predict(X_test))

# Using LassoCV function
from sklearn.linear_model import LassoCV
lasso_cv = linear_model.LassoCV(cv=10,fit_intercept=True)
lasso_cv.fit(X_train, y_train)
# Predict
lasso_cv_test_pred = lasso_cv.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,lasso_cv_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Compute mean squared error
MSE_lasso_cv = mean_squared_error(y_test,lasso_cv.predict(X_test))

lasso_cv_noint = linear_model.LassoCV(cv=10,fit_intercept=False)
lasso_cv_noint.fit(X_train, y_train)
# Predict
lasso_cv_noint_test_pred = lasso_cv_noint.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,lasso_cv_noint_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Compute mean squared error
MSE_lasso_cv_noint = mean_squared_error(y_test,lasso_cv_noint.predict(X_test))

########################################################################################################################

# Support Vector Regression

from sklearn import svm
svr = svm.SVR()
svr.fit(X_train, y_train) 
# Predict
svr_test_pred = svr.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,svr_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Performance 
MSE_svr = mean_squared_error(y_test,svr.predict(X_test)) 

# Using all variables
svr_full = svm.SVR()
svr_full.fit(X_full_train, y_train) 
# Predict
svr_full_test_pred = svr_full.predict(X_full_test)
# Plot
fig = plt.figure()
plt.plot(y_test,svr_full_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Performance 
MSE_svr_full = mean_squared_error(y_test,svr_full_test_pred)

#######################################################################################################################

# Random Forest Regression

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()
rf.fit(X_train, y_train) 
# Predict
rf_test_pred = rf.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,rf_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Performance 
MSE_rf = mean_squared_error(y_test,rf.predict(X_test)) 

rf_full = RandomForestRegressor()
rf_full.fit(X_full_train, y_train) 
# Predict
rf_full_test_pred = rf_full.predict(X_full_test)
# Plot
fig = plt.figure()
plt.plot(y_test,rf_full_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Performance 
MSE_rf_full = mean_squared_error(y_test,rf_full_test_pred) 

########################################################################################################################

# Bagging

from sklearn.ensemble import BaggingRegressor
bagg = BaggingRegressor()
bagg.fit(X_train, y_train) 
# Predict
bagg_test_pred = bagg.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,bagg_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Performance 
MSE_bagg = mean_squared_error(y_test,bagg.predict(X_test)) 

bagg_full = BaggingRegressor()
bagg_full.fit(X_full_train, y_train) 
# Predict
bagg_full_test_pred = bagg_full.predict(X_full_test)
# Plot
fig = plt.figure()
plt.plot(y_test,bagg_full_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Performance 
MSE_bagg_full = mean_squared_error(y_test,bagg_full_test_pred)

########################################################################################################################

# Gradient Boosting Regressor

from sklearn.ensemble import GradientBoostingRegressor
gbr = GradientBoostingRegressor()
gbr.fit(X_train, y_train) 
# Predict
gbr_test_pred = gbr.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,gbr_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Performance
MSE_gbr = mean_squared_error(y_test,gbr_test_pred)

gbr_full = GradientBoostingRegressor()
gbr_full.fit(X_full_train, y_train) 
# Predict
gbr_full_test_pred = gbr_full.predict(X_full_test)
# Plot
fig = plt.figure()
plt.plot(y_test,gbr_full_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Performance 
MSE_gbr_full = mean_squared_error(y_test,gbr_full_test_pred) 

########################################################################################################################

# AdaBoost Regressor

from sklearn.ensemble import AdaBoostRegressor
abr = AdaBoostRegressor()
abr.fit(X_train, y_train) 
# Predict
abr_test_pred = abr.predict(X_test)
# Plot
fig = plt.figure()
plt.plot(y_test,abr_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Performance 
MSE_abr = mean_squared_error(y_test,abr_test_pred) 

abr_full = AdaBoostRegressor()
abr_full.fit(X_full_train, y_train) 
# Predict
abr_full_test_pred = abr_full.predict(X_full_test)
# Plot
fig = plt.figure()
plt.plot(y_test,abr_full_test_pred,'kx')
plt.plot(plt.xlim(), plt.ylim(), ls="--")
# Performance 
MSE_abr_full = mean_squared_error(y_test,abr_full_test_pred)
 

########################################################################################################################
########################################################################################################################

# MODEL PERFORMANCE

########################################################################################################################
########################################################################################################################

# Testing the performance of the best model, chosen as the model with the lowest MSE

# One can look at how the chosen model performs on predicting the number of mails sent by a specific contributor.
# Specify the contributor who you wish to predict for.

# Actual number of mails sent by Contributor
y_value = y.ix[contributor][0]   
    
if min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_ridge_int:
    y_prediction = ridge_int.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print (""""The chosen model is a ridge regression model with an intercept; built with variables x3, x4, x5, x6 and x7. 
    
    Where:
    
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields
          
    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_ridge_noint:
    y_prediction = str(ridge_noint.predict((X.ix[contributor]).reshape(1,-1)))[2:][:-1]
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a ridge regression model without an intercept, built with variables x3, x4, x5, x6 and x7.
    
    Where:
    
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_ridgecv_int:
    y_prediction = ridgecv_int.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a ridge regression model with an intercept, where the optimal alpha value has been selected by 10-fold cross-validation, built with variables x3, x4, x5, x6 and x7.
    
    Where:
    
    x1: Average length of mail sent by contributor
    x2: IsBot - Indicator variable on whether the author is a Bot or not    
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    
    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_ridgecv_noint:
    y_prediction = ridgecv_noint.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a ridge regression model without an intercept, where the optimal alpha value has been selected by 10-fold cross-validation, built with variables x3, x4, x5, x6 and x7.
    
    Where:
        
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_lasso:
    y_prediction = lasso.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a lasso model with an intercept, built with variables x3, x4, x5, x6 and x7.
    
    Where:
      
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_lasso_noint:
    y_prediction = lasso_noint.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a lasso model without an intercept, built with variables x3, x4, x5, x6 and x7.
    
    Where:
      
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields
    
    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_lasso_cv:
    y_prediction = lasso_cv.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a lasso model with an intercept, where the optimal alpha value has been selected by 10-fold cross-validation. Built with variables x3, x4, x5, x6 and x7.
    
    Where:
      
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_lasso_cv_noint:
    y_prediction = lasso_cv_noint.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a lasso model without an intercept, where the optimal alpha value has been selected by 10-fold cross-validation, Built with variables x3, x4, x5, x6 and x7.
    
    Where:
      
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_svr:
    y_prediction = svr.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a Support Vector Regression model, built with variables x3, x4, x5, x6 and x7.
    
    Where:
      
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_svr_full:
    y_prediction = svr_full.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a Support Vector Regression model, built using all 7 variables.
    
    Where:
    
    x1: Average length of mail sent by contributor
    x2: IsBot - Indicator variable on whether the author is a Bot or not    
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_rf:
    y_prediction = rf.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a random forest model, built with variables x3, x4, x5, x6 and x7.
    
    Where:
      
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_rf_full:
    y_prediction = rf_full.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a random forest model, built using all 7 variables.
    
    Where:
    
    x1: Average length of mail sent by contributor
    x2: IsBot - Indicator variable on whether the author is a Bot or not    
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_bagg:
    y_prediction = bagg.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a bagging model, built with variables x3, x4, x5, x6 and x7.
    
    Where:
     
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_bagg_full:
    y_prediction = bagg_full.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a bagging model, built using all 7 variables.
    
    Where:
    
    x1: Average length of mail sent by contributor
    x2: IsBot - Indicator variable on whether the author is a Bot or not    
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_gbr:
    y_prediction = gbr.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a gradient boosting regressor model, built with variables x3, x4, x5, x6 and x7.
    
    Where:
       
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_gbr_full:
    y_prediction = gbr_full.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a gradient boosting regressor model, built using all 7 variables.
    
    Where:
    
    x1: Average length of mail sent by contributor
    x2: IsBot - Indicator variable on whether the author is a Bot or not    
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_abr:
    y_prediction = abr.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a AdaBoost regressor model, built with variables x3, x4, x5, x6 and x7.
    
    Where:
        
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

elif min(MSE_ridge_int, MSE_ridge_noint, MSE_ridgecv_int, MSE_ridgecv_noint, MSE_lasso, MSE_lasso_noint, MSE_lasso_cv, MSE_lasso_cv_noint, MSE_svr, MSE_svr_full, MSE_rf, MSE_rf_full, MSE_bagg, MSE_bagg_full, MSE_gbr, MSE_gbr_full, MSE_abr, MSE_abr_full) == MSE_abr_full:
    y_prediction = abr_full.predict((X.ix[contributor]).reshape(1,-1))
    squared_error = pow((y_prediction-y_value),2)

    print ("""The chosen model is a AdaBoost regressor model, built using all 7 variables.
    
    Where:
    
    x1: Average length of mail sent by contributor
    x2: IsBot - Indicator variable on whether the author is a Bot or not    
    x3: Number of mails containing a question in the body field
    x4: Number of mails containing a question in the subject field
    x5: Number of mails containing some form of "thanks" in the body field
    x6: Number of mails containing "fixed" in the body or subject fields
    x7: Number of mails containing a variation of 'commit'/'committer'/'comitted' in the body or subject fields

    The MSE for this model is {MSE}.
    
    The predicted number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_1} mails.
    The actual number of mails sent by {contrib} over the course of his/her lifetime to the {mail_list} is {num_of_mails_2} mails.
    
    The squared error for this prediction is {se}.""".format(MSE=MSE_ridge_int, contrib=contributor, mail_list=mailing_list, num_of_mails_1=str(y_prediction)[2:][:-1], num_of_mails_2=y_value, se=str(squared_error)[2:][:-1]))

                                 
                                                                                                                                                        

           
